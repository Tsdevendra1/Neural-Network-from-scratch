# Neural Network from scratch

This repository contains code for a neural network written from scratch. All derivations were done on paper and then coded to ensure correct implementation. 

So far the network includes:
  - Batch Norm
  - Adam Optimizer
  -Logistic regression loss function
  
To be added:
  -Dropout
  -Residual learning
  -Sigmoid loss function
  -RSS loss function
  
The network has only been evaluated to approximate the XOR function. 
  
