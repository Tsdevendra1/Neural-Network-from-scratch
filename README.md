# Neural Network from scratch

This repository contains code for a neural network written from scratch. All derivations were done on paper and then coded to ensure correct implementation. 

So far the network includes:
  - Batch Norm
  - Adam Optimizer
  - Logistic regression loss function
  - Multi class loss function
  
  
To be added:
  - Dropout
  - Residual Learning
  - RSS loss function
    
The network has only been evaluated to approximate the XOR function. 
  
